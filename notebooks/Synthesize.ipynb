{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adb7bd1",
   "metadata": {},
   "source": [
    "# StyleTTS 2 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49702a",
   "metadata": {},
   "source": [
    "### Interactive Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce343f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Settings & Data Sources\n",
    "# @markdown ***Language:***\n",
    "# @markdown <br><small>This language will be used to convert your text into phonemes.</small>\n",
    "# @markdown <br><small>This value must be one of the languages compatible with the phonemizer: https://pypi.org/project/phonemizer/</small>\n",
    "# @markdown <br><small>For example, if you're using the default espeak phonemization: https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md</small>\n",
    "language = 'en-us' # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ***Multi-speaker mode:***\n",
    "# @markdown <br><small>Tick the checkbox if you're using a multi-speaker model.</small>\n",
    "is_multispeaker = False # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown ***Number of diffusion steps:***\n",
    "# @markdown <br><small>The basic value is 5.</small>\n",
    "# @markdown <br><small>The more diffusion steps, the more diverse the output speech is at the expense of slow inference</small>\n",
    "diffusion_steps = 5 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ***Embedding scale:***\n",
    "# @markdown <br><small>This is the classifier-free guidance scale.</small>\n",
    "# @markdown <br><small>The higher the scale, the more conditional the style is to the input text and hence more emotional.</small>\n",
    "embedding_scale = 1 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ***Voice:***\n",
    "# @markdown <small>(choose a voice)</small>\n",
    "data_source = \"ThaTi\" # @param [\"ThaTi\", \"BarEm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108384d",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "\n",
    "# load packages\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ipywidgets as widgets\n",
    "import IPython.display as ipd\n",
    "import phonemizer\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "from text_utils import TextCleaner\n",
    "textcleaner = TextCleaner()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db3da4",
   "metadata": {},
   "source": [
    "### Randomness and GPU/CPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e173bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness settings\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1d122",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4a2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkce pro spuštění konkrétní buňky podle jejího čísla\n",
    "def run_cell(cell_num):\n",
    "    ipd.display(ipd.Javascript(f'Jupyter.notebook.execute_cells([{cell_num}])'))\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def compute_style(ref_dicts, model):\n",
    "    reference_embeddings = {}\n",
    "    for key, path in ref_dicts.items():\n",
    "        wave, sr = librosa.load(path, sr=24000)\n",
    "        audio, _ = librosa.effects.trim(wave, top_db=30)\n",
    "        if sr != 24000:\n",
    "            audio = librosa.resample(audio, sr, 24000)\n",
    "        mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ref = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        reference_embeddings[key] = (ref.squeeze(1), audio)\n",
    "    return reference_embeddings\n",
    "\n",
    "def fix_multispeaker(is_multispeaker, config):\n",
    "    if not is_multispeaker and config['model_params']['multispeaker']:\n",
    "        config['model_params']['multispeaker'] = False\n",
    "    return config\n",
    "\n",
    "def phonemize(text, phonemizer):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\"', '')\n",
    "    ps = phonemizer.phonemize([text])\n",
    "    return ps[0]\n",
    "\n",
    "def inference(ps, noise, vocoder, diffusion_steps=5, embedding_scale=1):\n",
    "    print(ps)\n",
    "    ps = word_tokenize(ps)\n",
    "    print(ps)\n",
    "    ps = ' '.join(ps)\n",
    "    print(ps)\n",
    "    ps = ps.replace(' .', '.')\n",
    "    ps = ps.replace(' ,', ',')\n",
    "    ps = ps.replace(' ;', ';')\n",
    "    ps = ps.replace(' :', ':')\n",
    "    print(ps)\n",
    "    \n",
    "    tokens = textcleaner(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Input token length\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
    "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise, \n",
    "            embedding=bert_dur[0].unsqueeze(0),\n",
    "            num_steps=diffusion_steps,\n",
    "            embedding_scale=embedding_scale\n",
    "        ).squeeze(0)\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_dur[-1] += 5\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if vocoder == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "        \n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if vocoder == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "        \n",
    "        out = model.decoder(\n",
    "            t_en @ pred_aln_trg.unsqueeze(0).to(device), \n",
    "            F0_pred,\n",
    "            N_pred,\n",
    "            ref.squeeze().unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "    return out.squeeze().cpu().numpy()\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80,\n",
    "    n_fft=2048,\n",
    "    win_length=1200,\n",
    "    hop_length=300)\n",
    "mean, std = -4, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bcf280",
   "metadata": {},
   "source": [
    "### Voices Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ceb0220",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "voices = {\n",
    "    'ThaTi': 'Models/ThaTi-5s8k_ft-LibriTTS_bs8.ml400/epochs_2nd_00043.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cecbe",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fc4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language=language, preserve_punctuation=True,  with_stress=True)\n",
    "\n",
    "# config = yaml.safe_load(open(\"Models/ThaTi-5s8k_ft-LibriTTS_bs8.ml400/config.yml\"))\n",
    "config = yaml.safe_load(open(\"Models/LJS_orig/config.yml\"))\n",
    "config = fix_multispeaker(is_multispeaker, config)\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT_mlng.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2002f6",
   "metadata": {},
   "source": [
    "### Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc18cf7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = build_model(recursive_munch(config['model_params']), text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]\n",
    "\n",
    "# params_whole = torch.load(\"Models/ThaTi-5s8k_ft-LibriTTS_bs8.ml400/epoch_2nd_00049.pth\", map_location='cpu')\n",
    "params_whole = torch.load(\"Models/LJS_orig/epoch_2nd_00100.pth\", map_location='cpu')\n",
    "params = params_whole['net']\n",
    "\n",
    "# Fix model\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]\n",
    "\n",
    "# Init sampler\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
    "    clamp=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803110e",
   "metadata": {},
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654cc2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ***Text to synthesize:***\n",
    "# @markdown <br><small>Write a text to synthesize.</small>\n",
    "text = '''StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human-level text to speech synthesis.''' # @param {type:\"string\"}\n",
    "# text = '''dˈuː juː hæv ˌɛni pˈeɪn?''' # @param {type:\"string\"}\n",
    "text = 'A big canvas tent was the first thing to come within his vision.'\n",
    "# text = 'ɐ bˈɪɡ kˈænvəs tˈɛnt wʌzðə fˈɜːst θˈɪŋ tə kˈʌm wɪðˌɪn hɪz vˈɪʒən.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11a92a-193f-4a6a-9735-9291beeb6b05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "start = time.time()\n",
    "noise = torch.randn(1,1,256).to(device)\n",
    "ps = phonemize(text, global_phonemizer)\n",
    "wav = inference(\n",
    "    ps,\n",
    "    noise,\n",
    "    config['model_params']['decoder']['type'],\n",
    "    diffusion_steps=diffusion_steps,\n",
    "    embedding_scale=embedding_scale,\n",
    ")\n",
    "rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "display(ipd.Audio(wav, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da84300",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create button\n",
    "button = widgets.Button(description=\"Synthesize\")\n",
    "\n",
    "# Map button to function\n",
    "button.on_click(lambda b: run_cell(9))  # Cell number to be launched\n",
    "\n",
    "# Zobrazení tlačítka\n",
    "ipd.display(button)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
